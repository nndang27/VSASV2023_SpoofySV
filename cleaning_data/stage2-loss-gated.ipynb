{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d295a25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T10:10:37.929789Z",
     "iopub.status.busy": "2023-11-01T10:10:37.929416Z",
     "iopub.status.idle": "2023-11-01T10:10:40.847071Z",
     "shell.execute_reply": "2023-11-01T10:10:40.845830Z"
    },
    "papermill": {
     "duration": 2.925821,
     "end_time": "2023-11-01T10:10:40.849404",
     "exception": false,
     "start_time": "2023-11-01T10:10:37.923583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/stage2-14/Loss-Gated-Learning /kaggle/working/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0628aa0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T10:10:40.859046Z",
     "iopub.status.busy": "2023-11-01T10:10:40.858707Z",
     "iopub.status.idle": "2023-11-01T10:10:56.284323Z",
     "shell.execute_reply": "2023-11-01T10:10:56.283406Z"
    },
    "papermill": {
     "duration": 15.43272,
     "end_time": "2023-11-01T10:10:56.286591",
     "exception": false,
     "start_time": "2023-11-01T10:10:40.853871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/stage2-14/Loss-Gated-Learning\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r ./utils/requirements.txt (line 1)) (1.23.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r ./utils/requirements.txt (line 2)) (1.11.2)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r ./utils/requirements.txt (line 3)) (1.2.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r ./utils/requirements.txt (line 4)) (4.66.1)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r ./utils/requirements.txt (line 5)) (0.15.1)\r\n",
      "Requirement already satisfied: soundfile in /opt/conda/lib/python3.10/site-packages (from -r ./utils/requirements.txt (line 6)) (0.12.1)\r\n",
      "Collecting faiss-gpu (from -r ./utils/requirements.txt (line 7))\r\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r ./utils/requirements.txt (line 3)) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r ./utils/requirements.txt (line 3)) (3.1.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->-r ./utils/requirements.txt (line 5)) (2.31.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchvision->-r ./utils/requirements.txt (line 5)) (2.0.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->-r ./utils/requirements.txt (line 5)) (9.5.0)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile->-r ./utils/requirements.txt (line 6)) (1.15.1)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile->-r ./utils/requirements.txt (line 6)) (2.21)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r ./utils/requirements.txt (line 5)) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r ./utils/requirements.txt (line 5)) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r ./utils/requirements.txt (line 5)) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r ./utils/requirements.txt (line 5)) (2023.7.22)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r ./utils/requirements.txt (line 5)) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r ./utils/requirements.txt (line 5)) (4.6.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r ./utils/requirements.txt (line 5)) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r ./utils/requirements.txt (line 5)) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchvision->-r ./utils/requirements.txt (line 5)) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchvision->-r ./utils/requirements.txt (line 5)) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchvision->-r ./utils/requirements.txt (line 5)) (1.3.0)\r\n",
      "Installing collected packages: faiss-gpu\r\n",
      "Successfully installed faiss-gpu-1.7.2\r\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/input/stage2-14/Loss-Gated-Learning\n",
    "!pip install -r ./utils/requirements.txt  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "583a6954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T10:10:56.302072Z",
     "iopub.status.busy": "2023-11-01T10:10:56.301472Z",
     "iopub.status.idle": "2023-11-01T10:10:56.308099Z",
     "shell.execute_reply": "2023-11-01T10:10:56.307017Z"
    },
    "papermill": {
     "duration": 0.016528,
     "end_time": "2023-11-01T10:10:56.310057",
     "exception": false,
     "start_time": "2023-11-01T10:10:56.293529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/stage2-14/Loss-Gated-Learning/Stage2\n"
     ]
    }
   ],
   "source": [
    "%cd Stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d22624e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T10:10:56.324736Z",
     "iopub.status.busy": "2023-11-01T10:10:56.324502Z",
     "iopub.status.idle": "2023-11-01T10:10:56.335662Z",
     "shell.execute_reply": "2023-11-01T10:10:56.334833Z"
    },
    "papermill": {
     "duration": 0.021388,
     "end_time": "2023-11-01T10:10:56.338041",
     "exception": false,
     "start_time": "2023-11-01T10:10:56.316653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/Loss-Gated-Learning/Stage2/dataLoader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/Loss-Gated-Learning/Stage2/dataLoader.py\n",
    "\n",
    "import glob, numpy, os, random, soundfile, torch, wave\n",
    "from scipy import signal\n",
    "from tools import *\n",
    "\n",
    "def get_Loader(args, dic_label = None, cluster_only = False):\n",
    "    # Get the loader for the cluster, batch_size is set as 1 to handlle the variable length input. Details see 1.2 part from here: https://github.com/TaoRuijie/TalkNet-ASD/blob/main/FAQ.md \n",
    "    clusterLoader = cluster_loader(**vars(args))\n",
    "    clusterLoader = torch.utils.data.DataLoader(clusterLoader, batch_size = 1, shuffle = True, num_workers = args.n_cpu, drop_last = False)\n",
    "\n",
    "    if cluster_only == True: # Only do clustering\n",
    "        return clusterLoader\n",
    "    # Get the loader for training\n",
    "    trainLoader = train_loader(dic_label = dic_label, **vars(args))\n",
    "    trainLoader = torch.utils.data.DataLoader(trainLoader, batch_size = args.batch_size, shuffle = True, num_workers = args.n_cpu, drop_last = True)\n",
    "\n",
    "    return trainLoader, clusterLoader\n",
    "\n",
    "class train_loader(object):\n",
    "    def __init__(self, train_list, train_path, musan_path, rir_path, max_frames, dic_label, **kwargs):\n",
    "        self.train_path = train_path\n",
    "        self.max_frames = max_frames * 160 + 240 # Length of segment for training\n",
    "        self.dic_label = dic_label # Pseudo labels dict\n",
    "        self.noisetypes = ['noise','speech','music']\n",
    "        self.noisesnr = {'noise':[0,15],'speech':[13,20],'music':[5,15]}\n",
    "        self.numnoise = {'noise':[1,1], 'speech':[3,8], 'music':[1,1]}\n",
    "        self.noiselist = {}\n",
    "        augment_files   = glob.glob(os.path.join(musan_path,'*/*/*/*.wav'))\n",
    "        for file in augment_files:\n",
    "            if file.split('/')[-3] not in self.noiselist:\n",
    "                self.noiselist[file.split('/')[-3]] = []\n",
    "            self.noiselist[file.split('/')[-3]].append(file)\n",
    "        self.rir_files  = glob.glob(os.path.join(rir_path,'*/*/*.wav'))\n",
    "        self.data_list = []\n",
    "        lines = open(train_list).read().splitlines()\n",
    "        for index, line in enumerate(lines):\n",
    "            file_name     = line.split()[1]\n",
    "            self.data_list.append(file_name)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file = self.data_list[index] # Get the filename\n",
    "        label = self.dic_label[file] # Load the pseudo label\n",
    "        segments = self.load_wav(file = file) # Load the augmented segment\n",
    "        segments = torch.FloatTensor(numpy.array(segments))\n",
    "        return segments, label\n",
    "\n",
    "    def load_wav(self, file):\n",
    "        utterance, _ = soundfile.read(os.path.join(self.train_path, file)) # Read the wav file\n",
    "        if utterance.shape[0] <= self.max_frames: # Padding if less than required length\n",
    "            shortage = self.max_frames - utterance.shape[0]\n",
    "            utterance = numpy.pad(utterance, (0, shortage), 'wrap')\n",
    "        startframe = random.choice(range(0, utterance.shape[0] - (self.max_frames))) # Choose the startframe randomly\n",
    "        segment = numpy.expand_dims(numpy.array(utterance[int(startframe):int(startframe)+self.max_frames]), axis = 0)\n",
    "\n",
    "        if random.random() <= 0.5:\n",
    "            segment = self.add_rev(segment, length = self.max_frames) # Rever\n",
    "        if random.random() <= 0.5:\n",
    "            segment = self.add_noise(segment, random.choice(['music', 'speech', 'noise']), length = self.max_frames) # Noise\n",
    "\n",
    "        return segment[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def add_rev(self, audio, length):\n",
    "        rir_file    = random.choice(self.rir_files)\n",
    "        rir, sr     = soundfile.read(rir_file)\n",
    "        rir         = numpy.expand_dims(rir.astype(numpy.float),0)\n",
    "        rir         = rir / numpy.sqrt(numpy.sum(rir**2))\n",
    "        return signal.convolve(audio, rir, mode='full')[:,:length]\n",
    "\n",
    "    def add_noise(self, audio, noisecat, length):\n",
    "        clean_db    = 10 * numpy.log10(numpy.mean(audio ** 2)+1e-4)\n",
    "        numnoise    = self.numnoise[noisecat]\n",
    "        # print(\"numnoise: \", numnoise)\n",
    "        # print(\"self.noiselist[noisecat]: \", self.noiselist[noisecat])\n",
    "        if len(self.noiselist[noisecat]) == 1:\n",
    "            noiselist   = random.sample(self.noiselist[noisecat], 1)\n",
    "        else:\n",
    "            noiselist   = random.sample(self.noiselist[noisecat], random.randint(numnoise[0],numnoise[1]))\n",
    "        noises = []\n",
    "        for noise in noiselist:\n",
    "            noiselength = wave.open(noise, 'rb').getnframes() # Read the length of the noise file\t\t\t\n",
    "            if noiselength <= length:\n",
    "                noiseaudio, _ = soundfile.read(noise)\n",
    "                noiseaudio = numpy.pad(noiseaudio, (0, length - noiselength), 'wrap')\n",
    "            else:\n",
    "                start_frame = numpy.int64(random.random()*(noiselength-length)) # If length is enough\n",
    "                noiseaudio, _ = soundfile.read(noise, start = start_frame, stop = start_frame + length) # Only read some part to improve speed\n",
    "            noiseaudio = numpy.stack([noiseaudio],axis=0)\n",
    "            noise_db = 10 * numpy.log10(numpy.mean(noiseaudio ** 2)+1e-4) \n",
    "            noisesnr   = random.uniform(self.noisesnr[noisecat][0],self.noisesnr[noisecat][1])\n",
    "            noises.append(numpy.sqrt(10 ** ((clean_db - noise_db - noisesnr) / 10)) * noiseaudio)\n",
    "        noise = numpy.sum(numpy.concatenate(noises,axis=0),axis=0,keepdims=True)\n",
    "        return noise + audio\n",
    "\n",
    "class cluster_loader(object):\n",
    "    def __init__(self, train_list, train_path, **kwargs):        \n",
    "        self.data_list, self.data_length, self.data_label = [], [], []\n",
    "        self.train_path = train_path\n",
    "        lines = open(train_list).read().splitlines()\n",
    "        # Get the ground-truth labels, that is used to compute the NMI for post-analyze.\n",
    "        dictkeys = list(set([x.split()[0] for x in lines]))\n",
    "        dictkeys.sort()\n",
    "        dictkeys = { key : ii for ii, key in enumerate(dictkeys) }\n",
    "\n",
    "        for lidx, line in enumerate(lines):\n",
    "            data = line.split()\n",
    "            file_name = data[1]\n",
    "            file_length = float(data[-1])\n",
    "            speaker_label = dictkeys[data[0]]\n",
    "            self.data_list.append(file_name)  # Filename\n",
    "            self.data_length.append(file_length) # Filelength\n",
    "            self.data_label.append(speaker_label) # GT Speaker label\n",
    "\n",
    "        # sort the training set by the length of the audios, audio with similar length are saved togethor.\n",
    "        inds = numpy.array(self.data_length).argsort()\n",
    "        self.data_list, self.data_length, self.data_label = numpy.array(self.data_list)[inds], \\\n",
    "                                                            numpy.array(self.data_length)[inds], \\\n",
    "                                                            numpy.array(self.data_label)[inds]\n",
    "        self.minibatch = []\n",
    "        start = 0\n",
    "        while True: # Genearte each minibatch, audio with similar length are saved togethor.\n",
    "            frame_length = self.data_length[start]\n",
    "            minibatch_size = max(1, int(1600 // frame_length)) \n",
    "            end = min(len(self.data_list), start + minibatch_size)\n",
    "            self.minibatch.append([self.data_list[start:end], frame_length, self.data_label[start:end]])\n",
    "            if end == len(self.data_list):\n",
    "                break\n",
    "            start = end\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_lists, frame_length, data_labels = self.minibatch[index] # Get one minibatch\n",
    "        filenames, labels, segments = [], [], []\n",
    "        for num in range(len(data_lists)):\n",
    "            filename = data_lists[num] # Read filename\n",
    "            label = data_labels[num] # Read GT label\n",
    "            audio, sr = soundfile.read(os.path.join(self.train_path, filename))\n",
    "            if len(audio) < int(frame_length * sr):\n",
    "                shortage    = int(frame_length * sr) - len(audio) + 1\n",
    "                audio       = numpy.pad(audio, (0, shortage), 'wrap')\n",
    "            audio = numpy.array(audio[:int(frame_length * sr)]) # Get clean utterance, better for clustering\n",
    "            segments.append(audio)\n",
    "            filenames.append(filename)\n",
    "            labels.append(label)\n",
    "        segments = torch.FloatTensor(numpy.array(segments))\n",
    "        return segments, filenames, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.minibatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe6c8d3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T10:10:56.355573Z",
     "iopub.status.busy": "2023-11-01T10:10:56.355302Z",
     "iopub.status.idle": "2023-11-01T10:10:56.367676Z",
     "shell.execute_reply": "2023-11-01T10:10:56.366656Z"
    },
    "papermill": {
     "duration": 0.023923,
     "end_time": "2023-11-01T10:10:56.369860",
     "exception": false,
     "start_time": "2023-11-01T10:10:56.345937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/Loss-Gated-Learning/Stage2/tools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/Loss-Gated-Learning/Stage2/tools.py\n",
    "\n",
    "import warnings, torch, os, math, numpy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from operator import itemgetter\n",
    "\n",
    "def print_write(type, text, score_file): # A helper function to print the text and write the log\n",
    "    if type == 'T': # Classification training without LGL (Baseline)\n",
    "        epoch, loss, acc, nselects = text\n",
    "        print(\"%d epoch, LOSS %f, ACC %.2f%%, nselects %.2f%%\\n\"%(epoch, loss, acc, nselects))\n",
    "        score_file.write(\"[T], %d epoch, LOSS %f, ACC %.2f%%, nselects %.2f%%\\n\"%(epoch, loss, acc, nselects))\t\n",
    "    elif type == 'L': # Classification training with LGL (Propose)\n",
    "        epoch, loss, acc, nselects, gate = text\n",
    "        print(\"%d epoch, LOSS %f, ACC %.2f%%, nselects %.2f%%, Gate %.1f \\n\"%(epoch, loss, acc, nselects, gate))\n",
    "        score_file.write(\"[L], %d epoch, LOSS %f, ACC %.2f%%, nselects %.2f%%, Gate %.1f \\n\"%(epoch, loss, acc, nselects, gate))\t\n",
    "    elif type == 'C': # Clustering step\n",
    "        epoch, NMI = text\n",
    "        print(\"%d epoch, NMI %.2f\\n\"%(epoch, NMI))\n",
    "        score_file.write(\"[C], %d epoch, NMI %.2f\\n\"%(epoch, NMI))\n",
    "    elif type == 'E': # Evaluation step\n",
    "        epoch, EER, minDCF = text\n",
    "        print(\"EER %2.2f%%, minDCF %2.3f%%\\n\"%(EER, minDCF))\n",
    "        score_file.write(\"[E], %d epoch, EER %2.2f%%, minDCF %2.3f%%\\n\"%(epoch, EER, minDCF))\n",
    "    score_file.flush()\n",
    "\n",
    "def check_clustering(score_path, LGL): # Read the score.txt file, judge the next stage\n",
    "    lines = open(score_path).read().splitlines()\n",
    "\n",
    "    if LGL == True: # For LGL, the order is \n",
    "        # Iteration 1: (C-T-T...-T-L-L...-L-) \n",
    "        # Iteration 2: (C-T-T...-T-L-L...-L-) \n",
    "        # ...\n",
    "        EERs_T, epochs_T, EERs_L, epochs_L = [], [], [], []\n",
    "        iteration = 0\n",
    "        train_type = 'T'\n",
    "        for line in lines:\n",
    "            if line.split(',')[0] == '[C]': # Clear all results after clustering\n",
    "                EERs_T, EERs_L, epochs_T, epochs_L = [], [], [], []\n",
    "                train_type = 'T'\n",
    "                iteration += 1\n",
    "            elif line.split(',')[0] == '[E]': # Save the evaluation result in this iteration\n",
    "                epoch = int(line.split(',')[1].split()[0])\n",
    "                EER = float(line.split(',')[-2].split()[-1][:-1])\n",
    "                if train_type == 'T':\n",
    "                    epochs_T.append(epoch) \n",
    "                    EERs_T.append(EER) # Result in [T]\n",
    "                elif train_type == 'L':\n",
    "                    epochs_L.append(epoch)\n",
    "                    EERs_L.append(EER) # Result in [L]\n",
    "            elif line.split(',')[0] == '[T]': # If the stage is [T], record it\n",
    "                train_type = 'T'\n",
    "            elif line.split(',')[0] == '[L]': # If the stage is [L], record it\n",
    "                train_type = 'L'\n",
    "\n",
    "        if train_type == 'T': # The stage is [T], so need to judge the next step is keeping [T]? Or do LGL for [L] ?\n",
    "            if len(EERs_T) < 4: # Too short training epoch, keep training\n",
    "                return 'T', None, None, iteration\n",
    "            else:\n",
    "                if EERs_T[-1] > min(EERs_T) and EERs_T[-2] > min(EERs_T) and EERs_T[-3] > min(EERs_T): # Get the best training result already, go LGL\n",
    "                    best_epoch = epochs_T[EERs_T.index(min(EERs_T))]\n",
    "                    next_epoch = epochs_T[-1]\n",
    "                    return 'L', best_epoch, next_epoch, iteration\n",
    "                else:\n",
    "                    return 'T', None, None, iteration # EER can still drop, keep training \n",
    "\n",
    "        elif train_type == 'L':\n",
    "            if len(EERs_L) < 4: # Too short training epoch, keep LGL training\n",
    "                return 'L', None, None, iteration\n",
    "            else:\n",
    "                if EERs_L[-1] > min(EERs_L) and EERs_L[-2] > min(EERs_L) and EERs_L[-3] > min(EERs_L): # Get the best LGL result already, go clustering\n",
    "                    best_epoch = epochs_L[EERs_L.index(min(EERs_L))]\n",
    "                    next_epoch = epochs_L[-1]\n",
    "                    return 'C', best_epoch, next_epoch, iteration # Clustering based on the best epoch is more robust\n",
    "                else:\n",
    "                    return 'L', None, None, iteration # EER can still drop, keep training \n",
    "\n",
    "    else: # Baseline approach without LGL\n",
    "        EERs_T, epochs_T = [], []\n",
    "        iteration = 0\n",
    "        for line in lines:\n",
    "            if line.split(',')[0] == '[C]': # Clear all results after clustering\n",
    "                EERs_T, epochs_T = [], []\n",
    "                iteration += 1\n",
    "            elif line.split(',')[0] == '[E]': # Save the evaluation result\n",
    "                epoch = int(line.split(',')[1].split()[0])\n",
    "                EER = float(line.split(',')[-2].split()[-1][:-1])\n",
    "                epochs_T.append(epoch)\n",
    "                EERs_T.append(EER)\n",
    "\n",
    "        if len(EERs_T) < 4: # Too short training epoch, keep training\n",
    "            return 'T', None, None, iteration\n",
    "        else:\n",
    "            if EERs_T[-1] > min(EERs_T) and EERs_T[-2] > min(EERs_T) and EERs_T[-3] > min(EERs_T): # Get the best training result, go clustering\n",
    "                best_epoch = epochs_T[EERs_T.index(min(EERs_T))]\n",
    "                next_epoch = epochs_T[-1]\n",
    "                return 'C', best_epoch, next_epoch, iteration\n",
    "            else:\n",
    "                return 'T', None, None, iteration # EER can still drop, keep training \n",
    "\n",
    "def tuneThresholdfromScore(scores, labels, target_fa, target_fr = None):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    tunedThreshold = [];\n",
    "    if target_fr:\n",
    "        for tfr in target_fr:\n",
    "            idx = numpy.nanargmin(numpy.absolute((tfr - fnr)))\n",
    "            tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n",
    "    for tfa in target_fa:\n",
    "        idx = numpy.nanargmin(numpy.absolute((tfa - fpr)))\n",
    "        tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n",
    "    idxE = numpy.nanargmin(numpy.absolute((fnr - fpr)))\n",
    "    eer  = max(fpr[idxE],fnr[idxE])*100\n",
    "\n",
    "    return tunedThreshold, eer, fpr, fnr\n",
    "\n",
    "# Creates a list of false-negative rates, a list of false-positive rates\n",
    "# and a list of decision thresholds that give those error-rates.\n",
    "def ComputeErrorRates(scores, labels):\n",
    "\n",
    "    # Sort the scores from smallest to largest, and also get the corresponding\n",
    "    # indexes of the sorted scores.  We will treat the sorted scores as the\n",
    "    # thresholds at which the the error-rates are evaluated.\n",
    "    sorted_indexes, thresholds = zip(*sorted(\n",
    "        [(index, threshold) for index, threshold in enumerate(scores)],\n",
    "        key=itemgetter(1)))\n",
    "    sorted_labels = []\n",
    "    labels = [labels[i] for i in sorted_indexes]\n",
    "    fnrs = []\n",
    "    fprs = []\n",
    "\n",
    "    # At the end of this loop, fnrs[i] is the number of errors made by\n",
    "    # incorrectly rejecting scores less than thresholds[i]. And, fprs[i]\n",
    "    # is the total number of times that we have correctly accepted scores\n",
    "    # greater than thresholds[i].\n",
    "    for i in range(0, len(labels)):\n",
    "        if i == 0:\n",
    "            fnrs.append(labels[i])\n",
    "            fprs.append(1 - labels[i])\n",
    "        else:\n",
    "            fnrs.append(fnrs[i-1] + labels[i])\n",
    "            fprs.append(fprs[i-1] + 1 - labels[i])\n",
    "    fnrs_norm = sum(labels)\n",
    "    fprs_norm = len(labels) - fnrs_norm\n",
    "\n",
    "    # Now divide by the total number of false negative errors to\n",
    "    # obtain the false positive rates across all thresholds\n",
    "    fnrs = [x / float(fnrs_norm) for x in fnrs]\n",
    "\n",
    "    # Divide by the total number of corret positives to get the\n",
    "    # true positive rate.  Subtract these quantities from 1 to\n",
    "    # get the false positive rates.\n",
    "    fprs = [1 - x / float(fprs_norm) for x in fprs]\n",
    "    return fnrs, fprs, thresholds\n",
    "\n",
    "# Computes the minimum of the detection cost function.  The comments refer to\n",
    "# equations in Section 3 of the NIST 2016 Speaker Recognition Evaluation Plan.\n",
    "def ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa):\n",
    "    min_c_det = float(\"inf\")\n",
    "    min_c_det_threshold = thresholds[0]\n",
    "    for i in range(0, len(fnrs)):\n",
    "        # See Equation (2).  it is a weighted sum of false negative\n",
    "        # and false positive errors.\n",
    "        c_det = c_miss * fnrs[i] * p_target + c_fa * fprs[i] * (1 - p_target)\n",
    "        if c_det < min_c_det:\n",
    "            min_c_det = c_det\n",
    "            min_c_det_threshold = thresholds[i]\n",
    "    # See Equations (3) and (4).  Now we normalize the cost.\n",
    "    c_def = min(c_miss * p_target, c_fa * (1 - p_target))\n",
    "    min_dcf = min_c_det / c_def\n",
    "    return min_dcf, min_c_det_threshold\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ce5b364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T10:10:56.386979Z",
     "iopub.status.busy": "2023-11-01T10:10:56.386658Z",
     "iopub.status.idle": "2023-11-01T10:10:56.396851Z",
     "shell.execute_reply": "2023-11-01T10:10:56.395801Z"
    },
    "papermill": {
     "duration": 0.021176,
     "end_time": "2023-11-01T10:10:56.398900",
     "exception": false,
     "start_time": "2023-11-01T10:10:56.377724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/Loss-Gated-Learning/Stage2/main_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/Loss-Gated-Learning/Stage2/main_train.py\n",
    "\n",
    "import os, argparse, pickle, glob\n",
    "from model import *\n",
    "from dataLoader import *\n",
    "from tools import *\n",
    "\n",
    "parser = argparse.ArgumentParser(description = \"Loss Gated Learning\")\n",
    "parser.add_argument('--n_cpu',        type=int, default=8)\n",
    "parser.add_argument('--max_frames',    type=int, default=180)\n",
    "parser.add_argument('--batch_size',   type=int, default=256)\n",
    "parser.add_argument('--init_model',   type=str, default=\"\")\n",
    "parser.add_argument('--save_path',    type=str, default=\"\")\n",
    "parser.add_argument('--train_list',   type=str, default=\"\",help='Path for Vox2 list, https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/train_list.txt')\n",
    "parser.add_argument('--val_list',     type=str, default=\"\", help='Path for Vox_O list, https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test2.txt')\n",
    "parser.add_argument('--train_path',   type=str, default=\"\", help='Path to the Vox2 set')\n",
    "parser.add_argument('--val_path',     type=str, default=\"\", help='Path to the Vox_O set')\n",
    "parser.add_argument('--musan_path',   type=str, default=\"\", help='Path to the musan set')\n",
    "parser.add_argument('--rir_path',     type=str, default=\"\", help='Path to the rir set')\n",
    "parser.add_argument('--lr',           type=float, default=0.001)\n",
    "parser.add_argument('--n_cluster',    type=int, default=4000, help='Number of clusters')\n",
    "parser.add_argument('--test_interval',type=int, default=1)\n",
    "parser.add_argument('--max_epoch',    type=int, default=100)\n",
    "parser.add_argument('--LGL',          dest='LGL', action='store_true', help='Use LGL or baseline only')\n",
    "args = parser.parse_args()\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "inf_max = 10**3\n",
    "if args.LGL:\n",
    "    gates = [1, 3, 3, 5, 6] # Set the gates in each iterations, which is different from our paper because we use stronger augmentation in dataloader\n",
    "else:\n",
    "    gates = [inf_max, inf_max, inf_max, inf_max, inf_max] # Set the gate as a very large value = No gate (baseline)\n",
    "\n",
    "args.model_folder = os.path.join(args.save_path, 'model') # Path for the saved models\n",
    "args.dic_folder   = os.path.join(args.save_path, 'dic') # Path for the saved pseudo label dic\n",
    "args.score_path   = os.path.join(args.save_path, 'score.txt') # Path for the score file\n",
    "os.makedirs(args.model_folder, exist_ok = True)\n",
    "os.makedirs(args.dic_folder, exist_ok = True)\n",
    "score_file = open(args.score_path, \"a+\")\n",
    "\n",
    "stage, best_epoch, next_epoch, iteration = check_clustering(args.score_path, args.LGL) # Check the state of this epoch\n",
    "print(stage, best_epoch, next_epoch, iteration)\n",
    "\n",
    "Trainer = trainer(**vars(args)) # Define the framework\n",
    "modelfiles = glob.glob('%s/model0*.model'%args.model_folder) # Go for all saved model\n",
    "modelfiles.sort()\n",
    "\n",
    "if len(modelfiles) >= 1: # Load the previous model\n",
    "    Trainer.load_parameters(modelfiles[-1])\n",
    "    args.epoch = int(os.path.splitext(os.path.basename(modelfiles[-1]))[0][6:]) + 1\n",
    "else:\n",
    "    args.epoch = 1 # Start from the first epoch\n",
    "    for items in vars(args): # Save the parameters in args\n",
    "        score_file.write('%s %s\\n'%(items, vars(args)[items]));\n",
    "    score_file.flush()\n",
    "\n",
    "if args.epoch == 1:\t# Do clustering in the first epoch\n",
    "    Trainer.load_parameters(args.init_model) # Load the init_model\n",
    "    clusterLoader = get_Loader(args, cluster_only = True) # Data Loader\n",
    "    dic_label, NMI = Trainer.cluster_network(loader = clusterLoader, n_cluster = args.n_cluster) # Do clustering\n",
    "    pickle.dump(dic_label, open(args.dic_folder + \"/label%04d.pkl\"%args.epoch, \"wb\")) # Save the pseudo labels\n",
    "    print_write(type = 'C', text = [args.epoch, NMI], score_file = score_file)\n",
    "\n",
    "labelfiles = glob.glob('%s/label0*.pkl'%args.dic_folder) # Read the last pseudo labels\n",
    "labelfiles.sort()\n",
    "dic_label = pickle.load(open(labelfiles[-1], \"rb\"))\n",
    "print(\"Dic %s loaded!\"%labelfiles[-1])\n",
    "trainLoader, clusterLoader = get_Loader(args, dic_label) # data loader with the pseduo labels\n",
    "\n",
    "while args.epoch <= args.max_epoch:\n",
    "    stage, best_epoch, next_epoch, iteration = check_clustering(args.score_path, args.LGL) # Check the state of this epoch\n",
    "    print(args.epoch, stage, best_epoch, next_epoch, iteration)\n",
    "    if stage == 'T': # Classification training\n",
    "        loss, acc, nselects = Trainer.train_network(epoch = args.epoch, loader = trainLoader, gate = inf_max)\n",
    "        print_write(type = 'T', text = [args.epoch, loss, acc, nselects], score_file = score_file)\n",
    "\n",
    "    elif stage == 'L': # LGL training\n",
    "        if best_epoch != None: # LGL start from the best model from 'T' stage\n",
    "            Trainer.load_parameters('%s/model0%03d.model'%(args.model_folder, best_epoch)) # Load the best model\n",
    "        loss, acc, nselects = Trainer.train_network(epoch = args.epoch, loader = trainLoader, gate = gates[iteration - 1])\n",
    "        print_write(type = 'L', text = [args.epoch, loss,  acc, nselects, gates[iteration - 1]], score_file = score_file)\n",
    "\n",
    "    elif stage == 'C': # Clustering\n",
    "        iteration += 1\n",
    "        if iteration > 5: # Maximun iteration is 5\n",
    "            quit()\n",
    "        Trainer.load_parameters('%s/model0%03d.model'%(args.model_folder, best_epoch)) # Load the best model\n",
    "        clusterLoader = get_Loader(args, cluster_only = True) # Cluster loader\n",
    "        dic_label, NMI = Trainer.cluster_network(loader = clusterLoader, n_cluster = args.n_cluster) # Clustering\n",
    "        args.epoch = next_epoch\n",
    "        print_write(type = 'C', text = [args.epoch, NMI], score_file = score_file)\n",
    "        pickle.dump(dic_label, open(args.dic_folder + \"/label%04d.pkl\"%args.epoch, \"wb\")) # Save the pseudo label dic\n",
    "        print(\"Dic %s loaded!\"%(args.dic_folder + \"/label%04d.pkl\"%args.epoch))\n",
    "        Trainer = trainer(**vars(args)) # Define the framework\n",
    "        Trainer.load_parameters(args.init_model) # Load the init_model\n",
    "        trainLoader, clusterLoader = get_Loader(args, dic_label) # Get new dataloader with new label dic\n",
    "\n",
    "    if args.epoch % args.test_interval == 0 and stage != 'C': # evaluation\n",
    "        Trainer.save_parameters(args.model_folder + \"/model%04d.model\"%args.epoch) # Save the model\n",
    "        EER, minDCF = Trainer.eval_network(**vars(args))\n",
    "        print_write(type = 'E', text = [args.epoch, EER, minDCF], score_file = score_file)\n",
    "\n",
    "    args.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17900e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T10:10:56.414957Z",
     "iopub.status.busy": "2023-11-01T10:10:56.414665Z",
     "iopub.status.idle": "2023-11-01T10:10:56.419948Z",
     "shell.execute_reply": "2023-11-01T10:10:56.419084Z"
    },
    "papermill": {
     "duration": 0.016461,
     "end_time": "2023-11-01T10:10:56.422871",
     "exception": false,
     "start_time": "2023-11-01T10:10:56.406410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import warnings, torch, os, math, numpy\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn import metrics\n",
      "from operator import itemgetter\n",
      "\n",
      "def print_write(type, text, score_file): # A helper function to print the text and write the log\n",
      "    if type == 'T': # Classification training without LGL (Baseline)\n",
      "        epoch, loss, acc, nselects = text\n",
      "        print(\"%d epoch, LOSS %f, ACC %.2f%%, nselects %.2f%%\\n\"%(epoch, loss, acc, nselects))\n",
      "        score_file.write(\"[T], %d epoch, LOSS %f, ACC %.2f%%, nselects %.2f%%\\n\"%(epoch, loss, acc, nselects))\t\n",
      "    elif type == 'L': # Classification training with LGL (Propose)\n",
      "        epoch, loss, acc, nselects, gate = text\n",
      "        print(\"%d epoch, LOSS %f, ACC %.2f%%, nselects %.2f%%, Gate %.1f \\n\"%(epoch, loss, acc, nselects, gate))\n",
      "        score_file.write(\"[L], %d epoch, LOSS %f, ACC %.2f%%, nselects %.2f%%, Gate %.1f \\n\"%(epoch, loss, acc, nselects, gate))\t\n",
      "    elif type == 'C': # Clustering step\n",
      "        epoch, NMI = text\n",
      "        print(\"%d epoch, NMI %.2f\\n\"%(epoch, NMI))\n",
      "        score_file.write(\"[C], %d epoch, NMI %.2f\\n\"%(epoch, NMI))\n",
      "    elif type == 'E': # Evaluation step\n",
      "        epoch, EER, minDCF = text\n",
      "        print(\"EER %2.2f%%, minDCF %2.3f%%\\n\"%(EER, minDCF))\n",
      "        score_file.write(\"[E], %d epoch, EER %2.2f%%, minDCF %2.3f%%\\n\"%(epoch, EER, minDCF))\n",
      "    score_file.flush()\n",
      "\n",
      "def check_clustering(score_path, LGL): # Read the score.txt file, judge the next stage\n",
      "    lines = open(score_path).read().splitlines()\n",
      "\n",
      "    if LGL == True: # For LGL, the order is \n",
      "        # Iteration 1: (C-T-T...-T-L-L...-L-) \n",
      "        # Iteration 2: (C-T-T...-T-L-L...-L-) \n",
      "        # ...\n",
      "        EERs_T, epochs_T, EERs_L, epochs_L = [], [], [], []\n",
      "        iteration = 0\n",
      "        train_type = 'T'\n",
      "        for line in lines:\n",
      "            if line.split(',')[0] == '[C]': # Clear all results after clustering\n",
      "                EERs_T, EERs_L, epochs_T, epochs_L = [], [], [], []\n",
      "                train_type = 'T'\n",
      "                iteration += 1\n",
      "            elif line.split(',')[0] == '[E]': # Save the evaluation result in this iteration\n",
      "                epoch = int(line.split(',')[1].split()[0])\n",
      "                EER = float(line.split(',')[-2].split()[-1][:-1])\n",
      "                if train_type == 'T':\n",
      "                    epochs_T.append(epoch) \n",
      "                    EERs_T.append(EER) # Result in [T]\n",
      "                elif train_type == 'L':\n",
      "                    epochs_L.append(epoch)\n",
      "                    EERs_L.append(EER) # Result in [L]\n",
      "            elif line.split(',')[0] == '[T]': # If the stage is [T], record it\n",
      "                train_type = 'T'\n",
      "            elif line.split(',')[0] == '[L]': # If the stage is [L], record it\n",
      "                train_type = 'L'\n",
      "\n",
      "        if train_type == 'T': # The stage is [T], so need to judge the next step is keeping [T]? Or do LGL for [L] ?\n",
      "            if len(EERs_T) < 4: # Too short training epoch, keep training\n",
      "                return 'T', None, None, iteration\n",
      "            else:\n",
      "                if EERs_T[-1] > min(EERs_T) and EERs_T[-2] > min(EERs_T) and EERs_T[-3] > min(EERs_T): # Get the best training result already, go LGL\n",
      "                    best_epoch = epochs_T[EERs_T.index(min(EERs_T))]\n",
      "                    next_epoch = epochs_T[-1]\n",
      "                    return 'L', best_epoch, next_epoch, iteration\n",
      "                else:\n",
      "                    return 'T', None, None, iteration # EER can still drop, keep training \n",
      "\n",
      "        elif train_type == 'L':\n",
      "            if len(EERs_L) < 4: # Too short training epoch, keep LGL training\n",
      "                return 'L', None, None, iteration\n",
      "            else:\n",
      "                if EERs_L[-1] > min(EERs_L) and EERs_L[-2] > min(EERs_L) and EERs_L[-3] > min(EERs_L): # Get the best LGL result already, go clustering\n",
      "                    best_epoch = epochs_L[EERs_L.index(min(EERs_L))]\n",
      "                    next_epoch = epochs_L[-1]\n",
      "                    return 'C', best_epoch, next_epoch, iteration # Clustering based on the best epoch is more robust\n",
      "                else:\n",
      "                    return 'L', None, None, iteration # EER can still drop, keep training \n",
      "\n",
      "    else: # Baseline approach without LGL\n",
      "        EERs_T, epochs_T = [], []\n",
      "        iteration = 0\n",
      "        for line in lines:\n",
      "            if line.split(',')[0] == '[C]': # Clear all results after clustering\n",
      "                EERs_T, epochs_T = [], []\n",
      "                iteration += 1\n",
      "            elif line.split(',')[0] == '[E]': # Save the evaluation result\n",
      "                epoch = int(line.split(',')[1].split()[0])\n",
      "                EER = float(line.split(',')[-2].split()[-1][:-1])\n",
      "                epochs_T.append(epoch)\n",
      "                EERs_T.append(EER)\n",
      "\n",
      "        if len(EERs_T) < 4: # Too short training epoch, keep training\n",
      "            return 'T', None, None, iteration\n",
      "        else:\n",
      "            if EERs_T[-1] > min(EERs_T) and EERs_T[-2] > min(EERs_T) and EERs_T[-3] > min(EERs_T): # Get the best training result, go clustering\n",
      "                best_epoch = epochs_T[EERs_T.index(min(EERs_T))]\n",
      "                next_epoch = epochs_T[-1]\n",
      "                return 'C', best_epoch, next_epoch, iteration\n",
      "            else:\n",
      "                return 'T', None, None, iteration # EER can still drop, keep training \n",
      "\n",
      "def tuneThresholdfromScore(scores, labels, target_fa, target_fr = None):\n",
      "    fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)\n",
      "    fnr = 1 - tpr\n",
      "    tunedThreshold = [];\n",
      "    if target_fr:\n",
      "        for tfr in target_fr:\n",
      "            idx = numpy.nanargmin(numpy.absolute((tfr - fnr)))\n",
      "            tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n",
      "    for tfa in target_fa:\n",
      "        idx = numpy.nanargmin(numpy.absolute((tfa - fpr)))\n",
      "        tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n",
      "    idxE = numpy.nanargmin(numpy.absolute((fnr - fpr)))\n",
      "    eer  = max(fpr[idxE],fnr[idxE])*100\n",
      "\n",
      "    return tunedThreshold, eer, fpr, fnr\n",
      "\n",
      "# Creates a list of false-negative rates, a list of false-positive rates\n",
      "# and a list of decision thresholds that give those error-rates.\n",
      "def ComputeErrorRates(scores, labels):\n",
      "\n",
      "    # Sort the scores from smallest to largest, and also get the corresponding\n",
      "    # indexes of the sorted scores.  We will treat the sorted scores as the\n",
      "    # thresholds at which the the error-rates are evaluated.\n",
      "    sorted_indexes, thresholds = zip(*sorted(\n",
      "        [(index, threshold) for index, threshold in enumerate(scores)],\n",
      "        key=itemgetter(1)))\n",
      "    sorted_labels = []\n",
      "    labels = [labels[i] for i in sorted_indexes]\n",
      "    fnrs = []\n",
      "    fprs = []\n",
      "\n",
      "    # At the end of this loop, fnrs[i] is the number of errors made by\n",
      "    # incorrectly rejecting scores less than thresholds[i]. And, fprs[i]\n",
      "    # is the total number of times that we have correctly accepted scores\n",
      "    # greater than thresholds[i].\n",
      "    for i in range(0, len(labels)):\n",
      "        if i == 0:\n",
      "            fnrs.append(labels[i])\n",
      "            fprs.append(1 - labels[i])\n",
      "        else:\n",
      "            fnrs.append(fnrs[i-1] + labels[i])\n",
      "            fprs.append(fprs[i-1] + 1 - labels[i])\n",
      "    fnrs_norm = sum(labels)\n",
      "    fprs_norm = len(labels) - fnrs_norm\n",
      "\n",
      "    # Now divide by the total number of false negative errors to\n",
      "    # obtain the false positive rates across all thresholds\n",
      "    fnrs = [x / float(fnrs_norm) for x in fnrs]\n",
      "\n",
      "    # Divide by the total number of corret positives to get the\n",
      "    # true positive rate.  Subtract these quantities from 1 to\n",
      "    # get the false positive rates.\n",
      "    fprs = [1 - x / float(fprs_norm) for x in fprs]\n",
      "    return fnrs, fprs, thresholds\n",
      "\n",
      "# Computes the minimum of the detection cost function.  The comments refer to\n",
      "# equations in Section 3 of the NIST 2016 Speaker Recognition Evaluation Plan.\n",
      "def ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa):\n",
      "    min_c_det = float(\"inf\")\n",
      "    min_c_det_threshold = thresholds[0]\n",
      "    for i in range(0, len(fnrs)):\n",
      "        # See Equation (2).  it is a weighted sum of false negative\n",
      "        # and false positive errors.\n",
      "        c_det = c_miss * fnrs[i] * p_target + c_fa * fprs[i] * (1 - p_target)\n",
      "        if c_det < min_c_det:\n",
      "            min_c_det = c_det\n",
      "            min_c_det_threshold = thresholds[i]\n",
      "    # See Equations (3) and (4).  Now we normalize the cost.\n",
      "    c_def = min(c_miss * p_target, c_fa * (1 - p_target))\n",
      "    min_dcf = min_c_det / c_def\n",
      "    return min_dcf, min_c_det_threshold\n",
      "\n",
      "def accuracy(output, target, topk=(1,)):\n",
      "\n",
      "    maxk = max(topk)\n",
      "    batch_size = target.size(0)\n",
      "    _, pred = output.topk(maxk, 1, True, True)\n",
      "    pred = pred.t()\n",
      "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
      "    res = []\n",
      "    for k in topk:\n",
      "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
      "        res.append(correct_k.mul_(100.0 / batch_size))\n",
      "\n",
      "    return res\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"/kaggle/working/Loss-Gated-Learning/Stage2/tools.py\",\"r\") as f:\n",
    "    a = f.read()\n",
    "    \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9955a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T10:10:56.439575Z",
     "iopub.status.busy": "2023-11-01T10:10:56.439295Z",
     "iopub.status.idle": "2023-11-01T15:30:12.512834Z",
     "shell.execute_reply": "2023-11-01T15:30:12.511575Z"
    },
    "papermill": {
     "duration": 19156.086122,
     "end_time": "2023-11-01T15:30:12.516838",
     "exception": false,
     "start_time": "2023-11-01T10:10:56.430716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "T None None 0\r\n",
      "Model /kaggle/input/stage2-14/Loss-Gated-Learning/Stage1/checkpoint/model/model000000046.model loaded!\r\n",
      "100%|█████████████████████████████████████████| 500/500 [07:01<00:00,  1.19it/s]\r\n",
      "1 epoch, NMI 80.62\r\n",
      "\r\n",
      "Dic /kaggle/working/exp/LGL/dic/label0001.pkl loaded!\r\n",
      "1 T None None 1\r\n",
      "\r\n",
      "1 epoch, LOSS 8.241371, ACC 7.85%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:52<00:00, 41.06it/s]\r\n",
      "EER 13.07%, minDCF 0.725%\r\n",
      "\r\n",
      "2 T None None 1\r\n",
      "\r\n",
      "2 epoch, LOSS 6.672605, ACC 15.47%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:38<00:00, 54.91it/s]\r\n",
      "EER 12.73%, minDCF 0.715%\r\n",
      "\r\n",
      "3 T None None 1\r\n",
      "\r\n",
      "3 epoch, LOSS 6.308904, ACC 18.42%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:38<00:00, 56.22it/s]\r\n",
      "EER 12.67%, minDCF 0.712%\r\n",
      "\r\n",
      "4 T None None 1\r\n",
      "\r\n",
      "4 epoch, LOSS 6.087600, ACC 19.91%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:38<00:00, 55.84it/s]\r\n",
      "EER 12.67%, minDCF 0.696%\r\n",
      "\r\n",
      "5 T None None 1\r\n",
      "\r\n",
      "5 epoch, LOSS 5.915566, ACC 20.90%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 57.08it/s]\r\n",
      "EER 12.69%, minDCF 0.713%\r\n",
      "\r\n",
      "6 T None None 1\r\n",
      "\r\n",
      "6 epoch, LOSS 5.813900, ACC 21.39%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 56.73it/s]\r\n",
      "EER 12.53%, minDCF 0.727%\r\n",
      "\r\n",
      "7 T None None 1\r\n",
      "\r\n",
      "7 epoch, LOSS 5.705522, ACC 21.56%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:38<00:00, 55.07it/s]\r\n",
      "EER 12.33%, minDCF 0.730%\r\n",
      "\r\n",
      "8 T None None 1\r\n",
      "\r\n",
      "8 epoch, LOSS 5.572169, ACC 21.77%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:38<00:00, 55.33it/s]\r\n",
      "EER 12.53%, minDCF 0.717%\r\n",
      "\r\n",
      "9 T None None 1\r\n",
      "\r\n",
      "9 epoch, LOSS 5.471002, ACC 21.97%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:39<00:00, 53.50it/s]\r\n",
      "EER 12.93%, minDCF 0.732%\r\n",
      "\r\n",
      "10 T None None 1\r\n",
      "\r\n",
      "10 epoch, LOSS 5.373804, ACC 21.74%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:41<00:00, 51.37it/s]\r\n",
      "EER 12.94%, minDCF 0.742%\r\n",
      "\r\n",
      "11 L 7 10 1\r\n",
      "Model /kaggle/working/exp/LGL/model/model0007.model loaded!\r\n",
      "\r\n",
      "11 epoch, LOSS 0.172016, ACC 99.20%, nselects 24.67%, Gate 1.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:39<00:00, 53.93it/s]\r\n",
      "EER 12.74%, minDCF 0.730%\r\n",
      "\r\n",
      "12 L None None 1\r\n",
      "\r\n",
      "12 epoch, LOSS 0.132540, ACC 99.14%, nselects 27.55%, Gate 1.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:36<00:00, 59.29it/s]\r\n",
      "EER 12.60%, minDCF 0.735%\r\n",
      "\r\n",
      "13 L None None 1\r\n",
      "\r\n",
      "13 epoch, LOSS 0.117115, ACC 99.14%, nselects 28.44%, Gate 1.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:46<00:00, 45.86it/s]\r\n",
      "EER 12.29%, minDCF 0.744%\r\n",
      "\r\n",
      "14 L None None 1\r\n",
      "\r\n",
      "14 epoch, LOSS 0.111353, ACC 98.95%, nselects 28.70%, Gate 1.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 57.51it/s]\r\n",
      "EER 12.67%, minDCF 0.733%\r\n",
      "\r\n",
      "15 L None None 1\r\n",
      "\r\n",
      "15 epoch, LOSS 0.103425, ACC 98.93%, nselects 29.11%, Gate 1.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:39<00:00, 54.40it/s]\r\n",
      "EER 12.82%, minDCF 0.708%\r\n",
      "\r\n",
      "16 L None None 1\r\n",
      "\r\n",
      "16 epoch, LOSS 0.097852, ACC 98.91%, nselects 29.52%, Gate 1.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:35<00:00, 60.00it/s]\r\n",
      "EER 12.93%, minDCF 0.741%\r\n",
      "\r\n",
      "17 C 13 16 1\r\n",
      "Model /kaggle/working/exp/LGL/model/model0013.model loaded!\r\n",
      "100%|█████████████████████████████████████████| 500/500 [04:50<00:00,  1.72it/s]\r\n",
      "16 epoch, NMI 82.10\r\n",
      "\r\n",
      "Dic /kaggle/working/exp/LGL/dic/label0016.pkl loaded!\r\n",
      "Model /kaggle/input/stage2-14/Loss-Gated-Learning/Stage1/checkpoint/model/model000000046.model loaded!\r\n",
      "17 T None None 2\r\n",
      "\r\n",
      "17 epoch, LOSS 7.989569, ACC 9.39%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:48<00:00, 43.89it/s]\r\n",
      "EER 13.14%, minDCF 0.735%\r\n",
      "\r\n",
      "18 T None None 2\r\n",
      "\r\n",
      "18 epoch, LOSS 6.416783, ACC 17.86%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:38<00:00, 55.93it/s]\r\n",
      "EER 12.73%, minDCF 0.733%\r\n",
      "\r\n",
      "19 T None None 2\r\n",
      "\r\n",
      "19 epoch, LOSS 6.025288, ACC 21.01%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 56.69it/s]\r\n",
      "EER 12.80%, minDCF 0.727%\r\n",
      "\r\n",
      "20 T None None 2\r\n",
      "\r\n",
      "20 epoch, LOSS 5.794955, ACC 22.65%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:38<00:00, 56.03it/s]\r\n",
      "EER 12.40%, minDCF 0.709%\r\n",
      "\r\n",
      "21 T None None 2\r\n",
      "\r\n",
      "21 epoch, LOSS 5.644558, ACC 23.63%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 57.62it/s]\r\n",
      "EER 12.41%, minDCF 0.719%\r\n",
      "\r\n",
      "22 T None None 2\r\n",
      "\r\n",
      "22 epoch, LOSS 5.521626, ACC 24.10%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 57.22it/s]\r\n",
      "EER 12.54%, minDCF 0.702%\r\n",
      "\r\n",
      "23 T None None 2\r\n",
      "\r\n",
      "23 epoch, LOSS 5.404070, ACC 24.55%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:36<00:00, 58.42it/s]\r\n",
      "EER 12.40%, minDCF 0.728%\r\n",
      "\r\n",
      "24 T None None 2\r\n",
      "\r\n",
      "24 epoch, LOSS 5.293977, ACC 24.70%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:38<00:00, 54.98it/s]\r\n",
      "EER 12.67%, minDCF 0.730%\r\n",
      "\r\n",
      "25 T None None 2\r\n",
      "\r\n",
      "25 epoch, LOSS 5.200445, ACC 24.85%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 57.01it/s]\r\n",
      "EER 13.20%, minDCF 0.728%\r\n",
      "\r\n",
      "26 T None None 2\r\n",
      "\r\n",
      "26 epoch, LOSS 5.111980, ACC 24.85%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 57.07it/s]\r\n",
      "EER 13.20%, minDCF 0.738%\r\n",
      "\r\n",
      "27 L 20 26 2\r\n",
      "Model /kaggle/working/exp/LGL/model/model0020.model loaded!\r\n",
      "\r\n",
      "27 epoch, LOSS 0.608494, ACC 76.68%, nselects 37.18%, Gate 3.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 57.51it/s]\r\n",
      "EER 12.24%, minDCF 0.741%\r\n",
      "\r\n",
      "28 L None None 2\r\n",
      "\r\n",
      "28 epoch, LOSS 0.504679, ACC 80.25%, nselects 38.45%, Gate 3.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 57.22it/s]\r\n",
      "EER 11.96%, minDCF 0.726%\r\n",
      "\r\n",
      "29 L None None 2\r\n",
      "\r\n",
      "29 epoch, LOSS 0.464208, ACC 81.63%, nselects 38.72%, Gate 3.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 56.62it/s]\r\n",
      "EER 12.13%, minDCF 0.742%\r\n",
      "\r\n",
      "30 L None None 2\r\n",
      "\r\n",
      "30 epoch, LOSS 0.442983, ACC 82.43%, nselects 39.35%, Gate 3.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:43<00:00, 48.91it/s]\r\n",
      "EER 12.48%, minDCF 0.767%\r\n",
      "\r\n",
      "31 L None None 2\r\n",
      "\r\n",
      "31 epoch, LOSS 0.420519, ACC 83.28%, nselects 39.67%, Gate 3.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:38<00:00, 56.08it/s]\r\n",
      "EER 13.50%, minDCF 0.790%\r\n",
      "\r\n",
      "32 C 28 31 2\r\n",
      "Model /kaggle/working/exp/LGL/model/model0028.model loaded!\r\n",
      "100%|█████████████████████████████████████████| 500/500 [04:54<00:00,  1.70it/s]\r\n",
      "31 epoch, NMI 81.82\r\n",
      "\r\n",
      "Dic /kaggle/working/exp/LGL/dic/label0031.pkl loaded!\r\n",
      "Model /kaggle/input/stage2-14/Loss-Gated-Learning/Stage1/checkpoint/model/model000000046.model loaded!\r\n",
      "32 T None None 3\r\n",
      "\r\n",
      "32 epoch, LOSS 7.977731, ACC 9.56%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:42<00:00, 50.50it/s]\r\n",
      "EER 13.27%, minDCF 0.715%\r\n",
      "\r\n",
      "33 T None None 3\r\n",
      "\r\n",
      "33 epoch, LOSS 6.420860, ACC 17.94%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 56.31it/s]\r\n",
      "EER 12.80%, minDCF 0.728%\r\n",
      "\r\n",
      "34 T None None 3\r\n",
      "\r\n",
      "34 epoch, LOSS 6.044094, ACC 20.97%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:42<00:00, 49.87it/s]\r\n",
      "EER 12.48%, minDCF 0.702%\r\n",
      "\r\n",
      "35 T None None 3\r\n",
      "\r\n",
      "35 epoch, LOSS 5.789680, ACC 22.92%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:36<00:00, 58.63it/s]\r\n",
      "EER 12.60%, minDCF 0.708%\r\n",
      "\r\n",
      "36 T None None 3\r\n",
      "\r\n",
      "36 epoch, LOSS 5.639800, ACC 23.64%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 56.40it/s]\r\n",
      "EER 12.80%, minDCF 0.717%\r\n",
      "\r\n",
      "37 T None None 3\r\n",
      "\r\n",
      "37 epoch, LOSS 5.501731, ACC 24.35%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 57.19it/s]\r\n",
      "EER 12.43%, minDCF 0.706%\r\n",
      "\r\n",
      "38 T None None 3\r\n",
      "\r\n",
      "38 epoch, LOSS 5.366706, ACC 24.74%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:36<00:00, 58.06it/s]\r\n",
      "EER 12.60%, minDCF 0.710%\r\n",
      "\r\n",
      "39 T None None 3\r\n",
      "\r\n",
      "39 epoch, LOSS 5.300319, ACC 25.07%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:40<00:00, 52.34it/s]\r\n",
      "EER 12.67%, minDCF 0.727%\r\n",
      "\r\n",
      "40 T None None 3\r\n",
      "\r\n",
      "40 epoch, LOSS 5.176860, ACC 25.13%, nselects 100.00%\r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:36<00:00, 58.35it/s]\r\n",
      "EER 13.27%, minDCF 0.728%\r\n",
      "\r\n",
      "41 L 37 40 3\r\n",
      "Model /kaggle/working/exp/LGL/model/model0037.model loaded!\r\n",
      "\r\n",
      "41 epoch, LOSS 0.631588, ACC 75.56%, nselects 38.89%, Gate 3.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:39<00:00, 54.37it/s]\r\n",
      "EER 12.53%, minDCF 0.756%\r\n",
      "\r\n",
      "42 L None None 3\r\n",
      "\r\n",
      "42 epoch, LOSS 0.514650, ACC 79.18%, nselects 39.93%, Gate 3.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 56.97it/s]\r\n",
      "EER 12.80%, minDCF 0.745%\r\n",
      "\r\n",
      "43 L None None 3\r\n",
      "\r\n",
      "43 epoch, LOSS 0.474481, ACC 80.75%, nselects 40.44%, Gate 3.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:44<00:00, 47.70it/s]\r\n",
      "EER 12.53%, minDCF 0.718%\r\n",
      "\r\n",
      "44 L None None 3\r\n",
      "\r\n",
      "44 epoch, LOSS 0.445006, ACC 81.75%, nselects 40.86%, Gate 3.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:42<00:00, 49.80it/s]\r\n",
      "EER 12.48%, minDCF 0.740%\r\n",
      "\r\n",
      "45 L None None 3\r\n",
      "\r\n",
      "45 epoch, LOSS 0.426742, ACC 82.46%, nselects 41.32%, Gate 3.0 \r\n",
      "\r\n",
      "100%|███████████████████████████████████████| 2138/2138 [00:37<00:00, 57.48it/s]\r\n",
      "EER 12.53%, minDCF 0.744%\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/Loss-Gated-Learning/Stage2/main_train.py --save_path /kaggle/working/exp/LGL \\\n",
    "--max_epoch 45 --batch_size 350 --lr 0.001 --train_list /kaggle/input/list-file/list_file/train.txt \\\n",
    "--val_list /kaggle/input/list-file/list_file/valid.txt --train_path /kaggle/input/data-cluster/data_cluster/data_set \\\n",
    "--val_path /kaggle/input/data-cluster/data_cluster/data_set --musan_path /kaggle/input/musan-noise \\\n",
    "--rir_path /kaggle/input/rirs-noises/RIRS_NOISES/simulated_rirs \\\n",
    "--init_model /kaggle/input/stage2-14/Loss-Gated-Learning/Stage1/checkpoint/model/model000000046.model \\\n",
    "--test_interval 1 --n_cluster 731 --LGL --max_frames 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aca647a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T15:30:18.287133Z",
     "iopub.status.busy": "2023-11-01T15:30:18.286763Z",
     "iopub.status.idle": "2023-11-01T15:30:18.291616Z",
     "shell.execute_reply": "2023-11-01T15:30:18.290771Z"
    },
    "papermill": {
     "duration": 2.790125,
     "end_time": "2023-11-01T15:30:18.293644",
     "exception": false,
     "start_time": "2023-11-01T15:30:15.503519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -r /kaggle/working/exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1fb5c",
   "metadata": {
    "papermill": {
     "duration": 2.831755,
     "end_time": "2023-11-01T15:30:23.857619",
     "exception": false,
     "start_time": "2023-11-01T15:30:21.025864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19192.402872,
   "end_time": "2023-11-01T15:30:27.034628",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-01T10:10:34.631756",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
