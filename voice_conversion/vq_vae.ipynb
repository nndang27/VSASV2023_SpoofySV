{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/NVIDIA/apex","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-22T00:02:33.792565Z","iopub.execute_input":"2023-10-22T00:02:33.793400Z","iopub.status.idle":"2023-10-22T00:02:36.506034Z","shell.execute_reply.started":"2023-10-22T00:02:33.793365Z","shell.execute_reply":"2023-10-22T00:02:36.504935Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'apex'...\nremote: Enumerating objects: 11396, done.\u001b[K\nremote: Counting objects: 100% (3464/3464), done.\u001b[K\nremote: Compressing objects: 100% (443/443), done.\u001b[K\nremote: Total 11396 (delta 3178), reused 3100 (delta 3019), pack-reused 7932\u001b[K\nReceiving objects: 100% (11396/11396), 15.39 MiB | 20.54 MiB/s, done.\nResolving deltas: 100% (8007/8007), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd apex","metadata":{"execution":{"iopub.status.busy":"2023-10-22T00:02:37.127510Z","iopub.execute_input":"2023-10-22T00:02:37.128398Z","iopub.status.idle":"2023-10-22T00:02:37.142423Z","shell.execute_reply.started":"2023-10-22T00:02:37.128363Z","shell.execute_reply":"2023-10-22T00:02:37.138287Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/apex\n","output_type":"stream"}]},{"cell_type":"code","source":"!git checkout 2386a912164b0c5cfcd8be7a2b890fbac5607c82","metadata":{"execution":{"iopub.status.busy":"2023-10-22T00:02:38.818041Z","iopub.execute_input":"2023-10-22T00:02:38.818425Z","iopub.status.idle":"2023-10-22T00:02:39.780859Z","shell.execute_reply.started":"2023-10-22T00:02:38.818395Z","shell.execute_reply":"2023-10-22T00:02:39.779622Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Note: switching to '2386a912164b0c5cfcd8be7a2b890fbac5607c82'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c <new-branch-name>\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 2386a91 Distributed optimizer infrastructure for FP8 parameters (#1723)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./","metadata":{"execution":{"iopub.status.busy":"2023-10-22T00:02:42.278374Z","iopub.execute_input":"2023-10-22T00:02:42.279163Z","iopub.status.idle":"2023-10-22T00:18:11.561075Z","shell.execute_reply.started":"2023-10-22T00:02:42.279130Z","shell.execute_reply":"2023-10-22T00:18:11.559879Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Using pip 23.1.2 from /opt/conda/lib/python3.10/site-packages/pip (python 3.10)\nProcessing /kaggle/working/apex\n  Running command Preparing metadata (pyproject.toml)\n\n\n  torch.__version__  = 2.0.0\n\n\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: packaging>20.6 in /opt/conda/lib/python3.10/site-packages (from apex==0.1) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>20.6->apex==0.1) (3.0.9)\nBuilding wheels for collected packages: apex\n  Running command Building wheel for apex (pyproject.toml)\n  /opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py:398: UserWarning: There are no g++ version bounds defined for CUDA version 11.8\n    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n  Emitting ninja build file /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n\n\n  torch.__version__  = 2.0.0\n\n\n\n  Compiling cuda extensions with\n  nvcc: NVIDIA (R) Cuda compiler driver\n  Copyright (c) 2005-2022 NVIDIA Corporation\n  Built on Wed_Sep_21_10:33:58_PDT_2022\n  Cuda compilation tools, release 11.8, V11.8.89\n  Build cuda_11.8.r11.8/compiler.31833905_0\n  from /usr/local/cuda/bin\n\n  [1/1] c++ -MMD -MF /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/flatten_unflatten.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/flatten_unflatten.cpp -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  Emitting ninja build file /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_l2norm_kernel_mp.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [2/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_l2norm_kernel.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [3/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_adagrad.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [4/14] c++ -MMD -MF /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/amp_C_frontend.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/amp_C_frontend.cpp -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  [5/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_axpby_kernel.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [6/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_adam.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [7/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_l2norm_scale_kernel.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [8/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_lamb_stage_1.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [9/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_lamb_stage_2.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [10/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_lamb.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [11/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_novograd.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [12/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_lamb_mp.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [13/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_sgd_kernel.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [14/14] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/multi_tensor_scale_kernel.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  Emitting ninja build file /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/syncbn.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/syncbn.cpp -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  [2/2] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/welford.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  Emitting ninja build file /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/layer_norm_cuda.cpp -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  [2/2] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/layer_norm_cuda_kernel.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  Emitting ninja build file /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/mlp.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/mlp.cpp -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  /kaggle/working/apex/csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n  /kaggle/working/apex/csrc/mlp.cpp:57:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n     57 |   for (int i = 0; i < num_layers; i++) {\n        |                   ~~^~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:64:76: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     64 |   auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n        |                                                              ~~~~~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/mlp.cpp:65:85: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     65 |   auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type());\n        |                                                                       ~~~~~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/mlp.cpp:67:58: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     67 |   auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n        |                                            ~~~~~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/ATen.h:11,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /kaggle/working/apex/csrc/mlp.cpp: In lambda function:\n  /kaggle/working/apex/csrc/mlp.cpp:69:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n        |                                       ~~~~~~~~~~~~~~^~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:228:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    228 |     const auto& the_type = TYPE;                                            \\\n        |                            ^~~~\n  /kaggle/working/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/ATen.h:11,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:231:47: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n    231 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:258:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n    258 |   AT_DISPATCH_SWITCH(                                        \\\n        |   ^~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:122:23: note: declared here\n    122 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n        |                       ^~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp: In lambda function:\n  /kaggle/working/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n     72 |     for (int i = 0; i < num_layers; i++) {\n        |                     ~~^~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n     78 |     auto result = mlp_fp<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp: In lambda function:\n  /kaggle/working/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n     72 |     for (int i = 0; i < num_layers; i++) {\n        |                     ~~^~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n     78 |     auto result = mlp_fp<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp: In lambda function:\n  /kaggle/working/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n     72 |     for (int i = 0; i < num_layers; i++) {\n        |                     ~~^~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n     78 |     auto result = mlp_fp<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n  /kaggle/working/apex/csrc/mlp.cpp:115:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n    115 |   for (int i = 0; i < num_layers; i++) {\n        |                   ~~^~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:120:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n    120 |   for (int i = 0; i < inputs.size(); i++) {\n        |                   ~~^~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:121:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    121 |     outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n        |                                                    ~~~~~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/ATen.h:11,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /kaggle/working/apex/csrc/mlp.cpp: In lambda function:\n  /kaggle/working/apex/csrc/mlp.cpp:124:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |                                       ~~~~~~~~~~~~~~^~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:228:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    228 |     const auto& the_type = TYPE;                                            \\\n        |                            ^~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/ATen.h:11,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:231:47: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n    231 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:258:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n    258 |   AT_DISPATCH_SWITCH(                                        \\\n        |   ^~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:122:23: note: declared here\n    122 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n        |                       ^~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp: In lambda function:\n  /kaggle/working/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n    126 |     for (int i = 0; i < num_layers; i++) {\n        |                     ~~^~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n    130 |     for (int i = 0; i < inputs.size(); i++) {\n        |                     ~~^~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n        |                                                                                    ~~~~~~~~~~~~~~^~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/ATen.h:11,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /kaggle/working/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n    140 |     auto result = mlp_bp<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:253:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    253 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp: In lambda function:\n  /kaggle/working/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n    126 |     for (int i = 0; i < num_layers; i++) {\n        |                     ~~^~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n    130 |     for (int i = 0; i < inputs.size(); i++) {\n        |                     ~~^~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n        |                                                                                    ~~~~~~~~~~~~~~^~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/ATen.h:11,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /kaggle/working/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n    140 |     auto result = mlp_bp<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:254:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    254 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp: In lambda function:\n  /kaggle/working/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n    126 |     for (int i = 0; i < num_layers; i++) {\n        |                     ~~^~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n    130 |     for (int i = 0; i < inputs.size(); i++) {\n        |                     ~~^~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n        |                                                                                    ~~~~~~~~~~~~~~^~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/ATen.h:11,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/mlp.cpp:1:\n  /kaggle/working/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n    140 |     auto result = mlp_bp<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:255:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    255 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:259:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n    259 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  [2/2] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/mlp_cuda.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  Emitting ninja build file /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/fused_dense.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/fused_dense.cpp -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  /kaggle/working/apex/csrc/fused_dense.cpp: In function ‘at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor)’:\n  /kaggle/working/apex/csrc/fused_dense.cpp:30:62: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     30 |   auto out = at::empty({batch_size, out_features}, input.type());\n        |                                                    ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:33:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     33 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n        |                                            ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/ATen.h:11,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n        |               ^~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n        |               ^~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n        |               ^~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n        |               ^~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor)’:\n  /kaggle/working/apex/csrc/fused_dense.cpp:64:68: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     64 |   auto d_weight = at::empty({out_features, in_features}, input.type());\n        |                                                          ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:68:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     68 |   auto d_bias = at::empty({out_features}, input.type());\n        |                                           ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:70:65: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     70 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n        |                                                       ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:73:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     73 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n        |                                            ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/ATen.h:11,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n        |               ^~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n        |               ^~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n        |               ^~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n        |               ^~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n  /kaggle/working/apex/csrc/fused_dense.cpp:106:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    106 |   auto output1 = at::empty({batch_size, hidden_features}, input.type());\n        |                                                           ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:107:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    107 |   auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n        |                                                           ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:108:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    108 |   auto output2 = at::empty({batch_size, out_features}, input.type());\n        |                                                        ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:111:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    111 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n        |                                            ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/ATen.h:11,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n  /kaggle/working/apex/csrc/fused_dense.cpp:149:72: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    149 |   auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n        |                                                              ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:150:73: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    150 |   auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n        |                                                               ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:151:57: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    151 |   auto d_bias1 = at::empty({hidden_features}, input.type());\n        |                                               ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:152:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    152 |   auto d_bias2 = at::empty({out_features}, input.type());\n        |                                            ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:153:65: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    153 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n        |                                                       ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:154:71: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    154 |   auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n        |                                                             ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:157:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n    157 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n        |                                            ~~~~~~~~~~^~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n    222 |   DeprecatedTypeProperties & type() const {\n        |                              ^~~~\n  In file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/ATen.h:11,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n                   from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:4,\n                   from /kaggle/working/apex/csrc/fused_dense.cpp:1:\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:246:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    246 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:247:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    247 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:272:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n    272 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:273:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    273 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp: In lambda function:\n  /kaggle/working/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n        |          ^~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:234:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n    234 |       __VA_ARGS__                                                           \\\n        |       ^~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:87:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n     87 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:274:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n    274 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n        |   ^~~~~~~~~~~~~~~~\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n    281 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  /kaggle/working/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  [2/2] /usr/local/cuda/bin/nvcc  -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/fused_dense_cuda.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  Emitting ninja build file /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/kaggle/working/apex/csrc -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  [2/2] /usr/local/cuda/bin/nvcc  -I/kaggle/working/apex/csrc -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  Emitting ninja build file /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/kaggle/working/apex/csrc -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/megatron/generic_scaled_masked_softmax.cpp -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  [2/2] /usr/local/cuda/bin/nvcc  -I/kaggle/working/apex/csrc -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/megatron/generic_scaled_masked_softmax_cuda.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/generic_scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  Emitting ninja build file /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/kaggle/working/apex/csrc -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/megatron/scaled_masked_softmax.cpp -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  [2/2] /usr/local/cuda/bin/nvcc  -I/kaggle/working/apex/csrc -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/megatron/scaled_masked_softmax_cuda.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  Emitting ninja build file /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/2] c++ -MMD -MF /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/kaggle/working/apex/csrc -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/megatron/scaled_softmax.cpp -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  [2/2] /usr/local/cuda/bin/nvcc  -I/kaggle/working/apex/csrc -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/megatron/scaled_softmax_cuda.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/scaled_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  Emitting ninja build file /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/build.ninja...\n  Compiling objects...\n  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n  [1/3] c++ -MMD -MF /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/kaggle/working/apex/csrc -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/megatron/fused_weight_gradient_dense.cpp -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  [2/3] /usr/local/cuda/bin/nvcc  -I/kaggle/working/apex/csrc -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  [3/3] /usr/local/cuda/bin/nvcc  -I/kaggle/working/apex/csrc -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/apex/csrc/megatron/fused_weight_gradient_dense_cuda.cu -o /kaggle/working/apex/build/temp.linux-x86_64-cpython-310/csrc/megatron/fused_weight_gradient_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/core/TensorImpl.h(77): here\n\n  /opt/conda/lib/python3.10/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison of unsigned integer with zero\n            detected during:\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  (61): here\n              instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\n  /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/qualified_name.h(73): here\n\n  Building wheel for apex (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for apex: filename=apex-0.1-cp310-cp310-linux_x86_64.whl size=5025446 sha256=0de2fce05da057b153ec95947d36bd5bd6e78baecfbb7966ac2ab024f3e28814\n  Stored in directory: /tmp/pip-ephem-wheel-cache-wwmeqx8r/wheels/ba/72/d0/acf80ec3f436eb539f498a28ef654232966163cc2f01a80dee\nSuccessfully built apex\nInstalling collected packages: apex\nSuccessfully installed apex-0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/input/vq-vae6/ZeroSpeech","metadata":{"execution":{"iopub.status.busy":"2023-10-22T00:18:11.563070Z","iopub.execute_input":"2023-10-22T00:18:11.563403Z","iopub.status.idle":"2023-10-22T00:18:11.577107Z","shell.execute_reply.started":"2023-10-22T00:18:11.563373Z","shell.execute_reply":"2023-10-22T00:18:11.576215Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/vq-vae6/ZeroSpeech\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-10-22T00:18:11.579925Z","iopub.execute_input":"2023-10-22T00:18:11.580191Z","iopub.status.idle":"2023-10-22T00:18:26.301780Z","shell.execute_reply.started":"2023-10-22T00:18:11.580168Z","shell.execute_reply":"2023-10-22T00:18:26.300790Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.11.2)\nRequirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.10.1)\nRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.57.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.66.1)\nCollecting hydra-core (from -r requirements.txt (line 6))\n  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyloudnorm (from -r requirements.txt (line 7))\n  Downloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.12.3)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 3)) (3.0.0)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 3)) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 3)) (1.3.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 3)) (5.1.1)\nRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 3)) (0.12.1)\nRequirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 3)) (1.7.0)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 3)) (0.3.6)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 3)) (4.6.3)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 3)) (0.2)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 3)) (1.0.5)\nRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->-r requirements.txt (line 4)) (0.40.1)\nCollecting omegaconf<2.4,>=2.2 (from hydra-core->-r requirements.txt (line 6))\n  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core->-r requirements.txt (line 6))\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from hydra-core->-r requirements.txt (line 6)) (21.3)\nRequirement already satisfied: future>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from pyloudnorm->-r requirements.txt (line 7)) (0.18.3)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (3.4.3)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (68.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (2.3.7)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (0.40.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (4.9)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (1.16.0)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 8)) (1.3.1)\nRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.4,>=2.2->hydra-core->-r requirements.txt (line 6)) (6.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa->-r requirements.txt (line 3)) (3.10.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->hydra-core->-r requirements.txt (line 6)) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 8)) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 8)) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 8)) (2023.7.22)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 3)) (1.15.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 8)) (2.1.3)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 3)) (2.21)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 8)) (3.2.2)\nBuilding wheels for collected packages: antlr4-python3-runtime\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=cad19c5e261ac11d9222a6cdfe98b1eca4f0c3473a46555f7c81591fad7cfc27\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built antlr4-python3-runtime\nInstalling collected packages: antlr4-python3-runtime, omegaconf, pyloudnorm, hydra-core\nSuccessfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0 pyloudnorm-0.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!python train.py hydra.run.dir=. hydra.output_subdir=null hydra/job_logging=disabled hydra/hydra_logging=disabled checkpoint_dir=/kaggle/working/checkpoints dataset=2019/english input_path=/kaggle/input/train-vq-vae-part2 speakers_file=speaker2.json train_file=train2.json num_spk=100 resume=/kaggle/input/vq-vae6/ZeroSpeech/checkpoints/part2/model_111.ckpt-107401.pt","metadata":{"execution":{"iopub.status.busy":"2023-10-22T00:18:26.304030Z","iopub.execute_input":"2023-10-22T00:18:26.304361Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/kaggle/input/vq-vae6/ZeroSpeech/preprocess.py:54: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=\"config\", config_name=\"preprocessing\")\n/kaggle/input/vq-vae6/ZeroSpeech/train.py:35: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  @hydra.main(config_path=\"config\", config_name=\"train\")\n/opt/conda/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n  warnings.warn(msg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n  ret = run_job(\nCONFIG:  {'dataset': {'dataset': {'dataset': 2019, 'language': 'english', 'path': '2019/english', 'n_speakers': 101}}, 'preprocessing': {'preprocessing': {'sr': 16000, 'n_fft': 2048, 'n_mels': 80, 'fmin': 50, 'preemph': 0.97, 'top_db': 80, 'hop_length': 160, 'win_length': 400, 'bits': 8}}, 'model': {'model': {'encoder': {'in_channels': 80, 'channels': 768, 'n_embeddings': 512, 'embedding_dim': 64, 'jitter': 0.5}, 'decoder': {'in_channels': 64, 'conditioning_channels': 128, 'n_speakers': 101, 'speaker_embedding_dim': 64, 'mu_embedding_dim': 256, 'rnn_channels': 896, 'fc_channels': 256, 'bits': 8, 'hop_length': 160}}}, 'training': {'training': {'batch_size': 8, 'sample_frames': 32, 'n_steps': 500000, 'optimizer': {'lr': 0.0004}, 'scheduler': {'milestones': [300000, 400000], 'gamma': 0.5}, 'checkpoint_interval': 1603, 'n_workers': 2}}, 'resume': '/kaggle/input/vq-vae6/ZeroSpeech/checkpoints/part2/model_111.ckpt-107401.pt', 'checkpoint_dir': '/kaggle/working/checkpoints', 'input_path': '/kaggle/input/train-vq-vae-part2', 'speakers_file': 'speaker2.json', 'train_file': 'train2.json', 'num_spk': 100}\nnumber of speaker:  100\n/opt/conda/lib/python3.10/site-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n  warnings.warn(msg, DeprecatedFeatureWarning)\nSelected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n\nDefaults for this optimization level are:\nenabled                : True\nopt_level              : O1\ncast_model_type        : None\npatch_torch_functions  : True\nkeep_batchnorm_fp32    : None\nmaster_weights         : None\nloss_scale             : dynamic\nProcessing user overrides (additional kwargs that are not None)...\nAfter processing overrides, optimization options are:\nenabled                : True\nopt_level              : O1\ncast_model_type        : None\npatch_torch_functions  : True\nkeep_batchnorm_fp32    : None\nmaster_weights         : None\nloss_scale             : dynamic\nResume checkpoint from: /kaggle/input/vq-vae6/ZeroSpeech/checkpoints/part2/model_111.ckpt-107401.pt:\nSpeakers JSON:  speaker2.json\nTrain JSON:  train2.json\nStart EPOCH:  112\nnumber of epoch:  521\n  0%|                                                   | 0/960 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/apex/amp/utils.py:203: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  w_fp16.set_(fp16_flat_tensor.storage(),\n 92%|█████████████████████████████████████▌   | 879/960 [09:02<00:49,  1.63it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:112, recon loss:5.56E-01, vq loss:2.34E-03, perpexlity:11.436\n 67%|███████████████████████████▍             | 642/960 [06:33<03:14,  1.63it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-109004.pt\n100%|█████████████████████████████████████████| 960/960 [09:48<00:00,  1.63it/s]\nepoch:113, recon loss:5.57E-01, vq loss:2.30E-03, perpexlity:11.144\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:114, recon loss:5.57E-01, vq loss:2.38E-03, perpexlity:11.102\n 34%|█████████████▉                           | 325/960 [03:20<06:30,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-110607.pt\n 45%|██████████████████▎                      | 429/960 [04:24<05:27,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:115, recon loss:5.56E-01, vq loss:2.59E-03, perpexlity:11.175\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:116, recon loss:5.58E-01, vq loss:2.55E-03, perpexlity:11.182\n  1%|▎                                          | 8/960 [00:05<09:56,  1.60it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-112210.pt\n 66%|███████████████████████████▏             | 637/960 [06:32<03:18,  1.63it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:117, recon loss:5.55E-01, vq loss:2.59E-03, perpexlity:10.892\n 68%|███████████████████████████▊             | 651/960 [06:41<03:10,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-113813.pt\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:118, recon loss:5.58E-01, vq loss:2.64E-03, perpexlity:10.797\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:119, recon loss:5.57E-01, vq loss:2.61E-03, perpexlity:10.597\n 35%|██████████████▎                          | 334/960 [03:26<06:25,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-115416.pt\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:120, recon loss:5.56E-01, vq loss:2.69E-03, perpexlity:10.548\n 26%|██████████▍                              | 245/960 [02:31<07:20,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:121, recon loss:5.55E-01, vq loss:2.69E-03, perpexlity:10.290\n  2%|▋                                         | 17/960 [00:10<09:39,  1.63it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-117019.pt\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:122, recon loss:5.55E-01, vq loss:2.78E-03, perpexlity:10.151\n 69%|████████████████████████████▏            | 660/960 [06:46<03:04,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-118622.pt\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:123, recon loss:5.55E-01, vq loss:2.82E-03, perpexlity:10.046\n 83%|█████████████████████████████████▉       | 796/960 [08:10<01:41,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:124, recon loss:5.57E-01, vq loss:3.33E-03, perpexlity:9.904\n 36%|██████████████▋                          | 343/960 [03:31<06:19,  1.63it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-120225.pt\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:125, recon loss:5.55E-01, vq loss:3.62E-03, perpexlity:9.936\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:126, recon loss:5.57E-01, vq loss:3.48E-03, perpexlity:9.987\n  3%|█▏                                        | 26/960 [00:16<09:35,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-121828.pt\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:127, recon loss:5.57E-01, vq loss:3.58E-03, perpexlity:10.027\n 70%|████████████████████████████▌            | 669/960 [06:52<02:59,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-123431.pt\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:128, recon loss:5.57E-01, vq loss:3.48E-03, perpexlity:9.727\n 81%|█████████████████████████████████▎       | 780/960 [08:01<01:50,  1.63it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:129, recon loss:5.55E-01, vq loss:3.63E-03, perpexlity:9.680\n 37%|███████████████                          | 352/960 [03:37<06:15,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-125034.pt\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:130, recon loss:5.55E-01, vq loss:3.65E-03, perpexlity:9.636\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:131, recon loss:5.56E-01, vq loss:3.46E-03, perpexlity:9.315\n  4%|█▌                                        | 35/960 [00:22<09:30,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-126637.pt\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:132, recon loss:5.56E-01, vq loss:3.31E-03, perpexlity:9.251\n 11%|████▎                                    | 101/960 [01:02<08:49,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n 71%|████████████████████████████▉            | 678/960 [06:58<02:53,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-128240.pt\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:133, recon loss:5.58E-01, vq loss:3.29E-03, perpexlity:8.983\n  4%|█▊                                        | 42/960 [00:26<09:26,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:134, recon loss:5.56E-01, vq loss:3.77E-03, perpexlity:8.644\n 38%|███████████████▍                         | 361/960 [03:43<06:08,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-129843.pt\n 46%|██████████████████▉                      | 443/960 [04:33<05:18,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:135, recon loss:5.55E-01, vq loss:3.66E-03, perpexlity:8.540\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:136, recon loss:5.56E-01, vq loss:3.43E-03, perpexlity:8.117\n  5%|█▉                                        | 44/960 [00:27<09:25,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-131446.pt\n 78%|████████████████████████████████         | 752/960 [07:44<02:08,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:137, recon loss:5.56E-01, vq loss:3.47E-03, perpexlity:8.547\n 72%|█████████████████████████████▎           | 687/960 [07:03<02:48,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-133049.pt\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:138, recon loss:5.56E-01, vq loss:3.54E-03, perpexlity:8.335\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:139, recon loss:5.57E-01, vq loss:3.21E-03, perpexlity:7.790\n 39%|███████████████▊                         | 370/960 [03:49<06:03,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-134652.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:140, recon loss:5.56E-01, vq loss:3.44E-03, perpexlity:7.730\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:141, recon loss:5.56E-01, vq loss:3.28E-03, perpexlity:7.172\n  6%|██▎                                       | 53/960 [00:33<09:19,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-136255.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:142, recon loss:5.57E-01, vq loss:3.10E-03, perpexlity:7.796\n 72%|█████████████████████████████▋           | 696/960 [07:09<02:42,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-137858.pt\n 92%|█████████████████████████████████████▋   | 881/960 [09:04<00:48,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:143, recon loss:5.57E-01, vq loss:3.65E-03, perpexlity:8.156\n100%|█████████████████████████████████████████| 960/960 [09:51<00:00,  1.62it/s]\nepoch:144, recon loss:5.58E-01, vq loss:3.13E-03, perpexlity:7.435\n  9%|███▋                                      | 84/960 [00:52<09:00,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n 39%|████████████████▏                        | 379/960 [03:54<05:58,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-139461.pt\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:145, recon loss:5.59E-01, vq loss:2.94E-03, perpexlity:7.707\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:146, recon loss:5.56E-01, vq loss:3.06E-03, perpexlity:6.729\n  6%|██▋                                       | 62/960 [00:38<09:13,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-141064.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:147, recon loss:5.57E-01, vq loss:3.84E-03, perpexlity:6.363\n 73%|██████████████████████████████           | 705/960 [07:15<02:37,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-142667.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:148, recon loss:5.56E-01, vq loss:3.04E-03, perpexlity:6.737\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:149, recon loss:5.58E-01, vq loss:2.76E-03, perpexlity:6.379\n  5%|██▏                                       | 49/960 [00:30<09:22,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n 40%|████████████████▌                        | 388/960 [03:59<05:53,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-144270.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:150, recon loss:5.59E-01, vq loss:2.31E-03, perpexlity:6.488\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:151, recon loss:5.58E-01, vq loss:2.18E-03, perpexlity:6.524\n  7%|███                                       | 71/960 [00:44<09:09,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-145873.pt\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:152, recon loss:5.58E-01, vq loss:1.79E-03, perpexlity:7.627\n 74%|██████████████████████████████▍          | 714/960 [07:20<02:31,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-147476.pt\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:153, recon loss:5.57E-01, vq loss:1.53E-03, perpexlity:7.731\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:154, recon loss:5.56E-01, vq loss:1.28E-03, perpexlity:7.312\n  3%|█▎                                        | 29/960 [00:18<09:36,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n 41%|████████████████▉                        | 397/960 [04:05<05:48,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-149079.pt\n 97%|███████████████████████████████████████▊ | 931/960 [09:34<00:17,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:155, recon loss:5.58E-01, vq loss:1.08E-03, perpexlity:7.729\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:156, recon loss:5.57E-01, vq loss:1.19E-03, perpexlity:8.117\n  8%|███▌                                      | 80/960 [00:49<09:05,  1.61it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-150682.pt\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:157, recon loss:5.57E-01, vq loss:1.02E-03, perpexlity:8.004\n 14%|█████▋                                   | 132/960 [01:21<08:30,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n 75%|██████████████████████████████▉          | 723/960 [07:25<02:25,  1.63it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-152285.pt\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:158, recon loss:5.59E-01, vq loss:8.68E-04, perpexlity:8.014\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:159, recon loss:5.57E-01, vq loss:8.94E-04, perpexlity:7.892\n 42%|█████████████████▎                       | 406/960 [04:10<05:41,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-153888.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:160, recon loss:5.58E-01, vq loss:8.15E-04, perpexlity:8.269\n 36%|██████████████▊                          | 348/960 [03:35<06:17,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:161, recon loss:5.56E-01, vq loss:5.71E-02, perpexlity:4.092\n  9%|███▉                                      | 89/960 [00:55<08:57,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-155491.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:162, recon loss:5.59E-01, vq loss:6.29E-02, perpexlity:1.803\n 76%|███████████████████████████████▎         | 732/960 [07:32<02:20,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-157094.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:163, recon loss:5.59E-01, vq loss:6.04E-02, perpexlity:1.605\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:164, recon loss:5.58E-01, vq loss:6.09E-02, perpexlity:1.541\n 29%|███████████▉                             | 280/960 [02:53<07:00,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n 43%|█████████████████▋                       | 415/960 [04:16<05:35,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-158697.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:165, recon loss:5.57E-01, vq loss:6.29E-02, perpexlity:1.625\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:166, recon loss:5.58E-01, vq loss:6.19E-02, perpexlity:1.885\n 10%|████▎                                     | 98/960 [01:01<08:52,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-160300.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:167, recon loss:5.56E-01, vq loss:5.29E-02, perpexlity:2.462\n 77%|███████████████████████████████▋         | 741/960 [07:38<02:15,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-161903.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:168, recon loss:5.56E-01, vq loss:5.79E-02, perpexlity:2.183\n 32%|█████████████▎                           | 311/960 [03:12<06:40,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n 65%|██████████████████████████▌              | 621/960 [06:23<03:28,  1.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n100%|█████████████████████████████████████████| 960/960 [09:52<00:00,  1.62it/s]\nepoch:169, recon loss:5.57E-01, vq loss:6.15E-02, perpexlity:2.001\n 44%|██████████████████                       | 424/960 [04:22<05:30,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-163506.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:170, recon loss:5.57E-01, vq loss:5.60E-02, perpexlity:2.106\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:171, recon loss:5.57E-01, vq loss:6.36E-02, perpexlity:2.228\n 11%|████▌                                    | 107/960 [01:06<08:47,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-165109.pt\n100%|█████████████████████████████████████████| 960/960 [09:53<00:00,  1.62it/s]\nepoch:172, recon loss:5.56E-01, vq loss:6.57E-02, perpexlity:3.109\n 78%|████████████████████████████████         | 750/960 [07:43<02:09,  1.62it/s]Saved checkpoint: /kaggle/working/checkpoints/model.ckpt-166712.pt\n 88%|████████████████████████████████████▏    | 847/960 [08:43<01:09,  1.62it/s]","output_type":"stream"}]}]}